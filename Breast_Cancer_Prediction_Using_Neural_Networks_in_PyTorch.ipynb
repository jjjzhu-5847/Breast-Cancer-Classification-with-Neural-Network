{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4Ot06MkxyP8jfYeajlvET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjjzhu-5847/Breast-Cancer-Classification-with-Neural-Network/blob/main/Breast_Cancer_Prediction_Using_Neural_Networks_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import the library**"
      ],
      "metadata": {
        "id": "fT5cnVQeuuQL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "5OoBGd6TrR2L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn # nn stands for neural network\n",
        "import torch.optim as optim # optim for optimizer\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer # load dataset library\n",
        "from sklearn.preprocessing import StandardScaler # standardize data\n",
        "from sklearn.model_selection import train_test_split # split data set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Device Configuration**"
      ],
      "metadata": {
        "id": "qnBxCKS0v9qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for CUDA availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # let pytorch now what architecture we are using\n",
        "print(f'the architecture we are using is: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puN2cfp0v4X9",
        "outputId": "2ac92fd0-0e15-4994-d5b0-03a1b74e3528"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the architecture we are using is: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preprocessing**"
      ],
      "metadata": {
        "id": "1FsCk5W5xScQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "x = data.data\n",
        "y = data.target"
      ],
      "metadata": {
        "id": "luORH0PexPgW"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djU1qzkoyFts",
        "outputId": "5debc146-bd1b-42f3-f0d1-a18a2ce503bd"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
            " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
            " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
            " ...\n",
            " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
            " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
            " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPFlUQNyIyR",
        "outputId": "995d59de-a8c6-4a1f-ee0b-9ea38ae3fd46"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into trainning data and testing data\n",
        "# 20% of data will be testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
        "\n",
        "print(x.shape)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd9pjrJsyL5T",
        "outputId": "06d0a6d8-0412-41ad-a36d-30bcb67d51b4"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "(455, 30)\n",
            "(114, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standarlize the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit will get the mean and sd of all precidtors\n",
        "# transform will apply the calculation to normal distubution\n",
        "\n",
        "# Reason why we use fit for train not test:\n",
        "# Because Data Leakage, if we use fit on test, then the test set will be standarlized base on known test set information\n",
        "x_train_std = scaler.fit_transform(x_train)\n",
        "x_test_std = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "sUl9Uyyxy3ZC"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converty numPy to PyTorch tensor and put it to GPU device\n",
        "x_train_std = torch.tensor(x_train_std, dtype=torch.float32).to(device)\n",
        "x_test_std = torch.tensor(x_test_std, dtype=torch.float32).to(device)\n",
        "y_train_std = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "y_test_std = torch.tensor(y_test, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "z41ZP26A5WGr"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Neural Networks**"
      ],
      "metadata": {
        "id": "QnKBabPT7x46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "4MDGVFah7m-K"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "input_size = x_train.shape[1]\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "Z_vYJ4MeEdQ5"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the neural network and put it to GPU device\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "id": "N3zL29_eHLt8"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.BCELoss()"
      ],
      "metadata": {
        "id": "DPglLEKrHeWb"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trainning Neural Network**"
      ],
      "metadata": {
        "id": "t1pyd01aJNfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  model.train() # set the model to trainning model, so it will optimize the paramaters\n",
        "  optimizer.zero_grad() # reset the gradient from the last epoch\n",
        "  outputs = model(x_train_std) # it will do forward papogation, forward() will be use inplictly\n",
        "  loss = loss_function(outputs, y_train_std.view(-1,1)) # calculate the loss between output and label(need to be reshape[size,1])\n",
        "  loss.backward() # backward propagation to get gradient\n",
        "  optimizer.step() # apply optimizer\n",
        "\n",
        "  # show the accuracy\n",
        "  # temly close gradient calculation\n",
        "  with torch.no_grad():\n",
        "    predicted = outputs.round() # round the outputs, trashhold is 0.5\n",
        "    accuracy = (predicted == y_train_std.view(-1,1)).float().mean() # compare all boolean value, change it to float, cal mean\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkDc8AKnJKYv",
        "outputId": "dcc1b149-9dbe-46f3-d8fc-6cf1b9c849ed"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.6673, Accuracy: 0.6374\n",
            "Epoch 2/100, Loss: 0.6484, Accuracy: 0.6967\n",
            "Epoch 3/100, Loss: 0.6301, Accuracy: 0.7516\n",
            "Epoch 4/100, Loss: 0.6123, Accuracy: 0.7846\n",
            "Epoch 5/100, Loss: 0.5950, Accuracy: 0.8088\n",
            "Epoch 6/100, Loss: 0.5784, Accuracy: 0.8286\n",
            "Epoch 7/100, Loss: 0.5622, Accuracy: 0.8549\n",
            "Epoch 8/100, Loss: 0.5465, Accuracy: 0.8659\n",
            "Epoch 9/100, Loss: 0.5313, Accuracy: 0.8725\n",
            "Epoch 10/100, Loss: 0.5167, Accuracy: 0.8725\n",
            "Epoch 11/100, Loss: 0.5025, Accuracy: 0.8813\n",
            "Epoch 12/100, Loss: 0.4888, Accuracy: 0.8835\n",
            "Epoch 13/100, Loss: 0.4756, Accuracy: 0.8879\n",
            "Epoch 14/100, Loss: 0.4628, Accuracy: 0.8879\n",
            "Epoch 15/100, Loss: 0.4505, Accuracy: 0.8945\n",
            "Epoch 16/100, Loss: 0.4386, Accuracy: 0.8945\n",
            "Epoch 17/100, Loss: 0.4271, Accuracy: 0.8945\n",
            "Epoch 18/100, Loss: 0.4160, Accuracy: 0.8989\n",
            "Epoch 19/100, Loss: 0.4053, Accuracy: 0.8989\n",
            "Epoch 20/100, Loss: 0.3950, Accuracy: 0.9011\n",
            "Epoch 21/100, Loss: 0.3850, Accuracy: 0.9033\n",
            "Epoch 22/100, Loss: 0.3754, Accuracy: 0.9033\n",
            "Epoch 23/100, Loss: 0.3661, Accuracy: 0.9033\n",
            "Epoch 24/100, Loss: 0.3572, Accuracy: 0.9033\n",
            "Epoch 25/100, Loss: 0.3485, Accuracy: 0.9055\n",
            "Epoch 26/100, Loss: 0.3402, Accuracy: 0.9121\n",
            "Epoch 27/100, Loss: 0.3321, Accuracy: 0.9121\n",
            "Epoch 28/100, Loss: 0.3244, Accuracy: 0.9165\n",
            "Epoch 29/100, Loss: 0.3169, Accuracy: 0.9165\n",
            "Epoch 30/100, Loss: 0.3096, Accuracy: 0.9187\n",
            "Epoch 31/100, Loss: 0.3026, Accuracy: 0.9187\n",
            "Epoch 32/100, Loss: 0.2959, Accuracy: 0.9209\n",
            "Epoch 33/100, Loss: 0.2894, Accuracy: 0.9209\n",
            "Epoch 34/100, Loss: 0.2831, Accuracy: 0.9209\n",
            "Epoch 35/100, Loss: 0.2770, Accuracy: 0.9231\n",
            "Epoch 36/100, Loss: 0.2712, Accuracy: 0.9231\n",
            "Epoch 37/100, Loss: 0.2655, Accuracy: 0.9231\n",
            "Epoch 38/100, Loss: 0.2601, Accuracy: 0.9253\n",
            "Epoch 39/100, Loss: 0.2548, Accuracy: 0.9275\n",
            "Epoch 40/100, Loss: 0.2497, Accuracy: 0.9275\n",
            "Epoch 41/100, Loss: 0.2447, Accuracy: 0.9297\n",
            "Epoch 42/100, Loss: 0.2399, Accuracy: 0.9341\n",
            "Epoch 43/100, Loss: 0.2353, Accuracy: 0.9385\n",
            "Epoch 44/100, Loss: 0.2308, Accuracy: 0.9407\n",
            "Epoch 45/100, Loss: 0.2265, Accuracy: 0.9407\n",
            "Epoch 46/100, Loss: 0.2223, Accuracy: 0.9407\n",
            "Epoch 47/100, Loss: 0.2183, Accuracy: 0.9429\n",
            "Epoch 48/100, Loss: 0.2143, Accuracy: 0.9429\n",
            "Epoch 49/100, Loss: 0.2105, Accuracy: 0.9429\n",
            "Epoch 50/100, Loss: 0.2069, Accuracy: 0.9451\n",
            "Epoch 51/100, Loss: 0.2033, Accuracy: 0.9451\n",
            "Epoch 52/100, Loss: 0.1998, Accuracy: 0.9451\n",
            "Epoch 53/100, Loss: 0.1965, Accuracy: 0.9451\n",
            "Epoch 54/100, Loss: 0.1932, Accuracy: 0.9451\n",
            "Epoch 55/100, Loss: 0.1901, Accuracy: 0.9473\n",
            "Epoch 56/100, Loss: 0.1870, Accuracy: 0.9495\n",
            "Epoch 57/100, Loss: 0.1840, Accuracy: 0.9495\n",
            "Epoch 58/100, Loss: 0.1811, Accuracy: 0.9495\n",
            "Epoch 59/100, Loss: 0.1783, Accuracy: 0.9495\n",
            "Epoch 60/100, Loss: 0.1756, Accuracy: 0.9516\n",
            "Epoch 61/100, Loss: 0.1730, Accuracy: 0.9516\n",
            "Epoch 62/100, Loss: 0.1704, Accuracy: 0.9516\n",
            "Epoch 63/100, Loss: 0.1679, Accuracy: 0.9516\n",
            "Epoch 64/100, Loss: 0.1655, Accuracy: 0.9516\n",
            "Epoch 65/100, Loss: 0.1631, Accuracy: 0.9538\n",
            "Epoch 66/100, Loss: 0.1609, Accuracy: 0.9538\n",
            "Epoch 67/100, Loss: 0.1586, Accuracy: 0.9538\n",
            "Epoch 68/100, Loss: 0.1565, Accuracy: 0.9560\n",
            "Epoch 69/100, Loss: 0.1543, Accuracy: 0.9560\n",
            "Epoch 70/100, Loss: 0.1523, Accuracy: 0.9604\n",
            "Epoch 71/100, Loss: 0.1503, Accuracy: 0.9626\n",
            "Epoch 72/100, Loss: 0.1483, Accuracy: 0.9626\n",
            "Epoch 73/100, Loss: 0.1464, Accuracy: 0.9648\n",
            "Epoch 74/100, Loss: 0.1446, Accuracy: 0.9670\n",
            "Epoch 75/100, Loss: 0.1428, Accuracy: 0.9670\n",
            "Epoch 76/100, Loss: 0.1410, Accuracy: 0.9670\n",
            "Epoch 77/100, Loss: 0.1393, Accuracy: 0.9670\n",
            "Epoch 78/100, Loss: 0.1376, Accuracy: 0.9670\n",
            "Epoch 79/100, Loss: 0.1360, Accuracy: 0.9692\n",
            "Epoch 80/100, Loss: 0.1344, Accuracy: 0.9692\n",
            "Epoch 81/100, Loss: 0.1328, Accuracy: 0.9714\n",
            "Epoch 82/100, Loss: 0.1313, Accuracy: 0.9714\n",
            "Epoch 83/100, Loss: 0.1298, Accuracy: 0.9714\n",
            "Epoch 84/100, Loss: 0.1284, Accuracy: 0.9714\n",
            "Epoch 85/100, Loss: 0.1270, Accuracy: 0.9714\n",
            "Epoch 86/100, Loss: 0.1256, Accuracy: 0.9714\n",
            "Epoch 87/100, Loss: 0.1243, Accuracy: 0.9714\n",
            "Epoch 88/100, Loss: 0.1229, Accuracy: 0.9714\n",
            "Epoch 89/100, Loss: 0.1217, Accuracy: 0.9714\n",
            "Epoch 90/100, Loss: 0.1204, Accuracy: 0.9714\n",
            "Epoch 91/100, Loss: 0.1192, Accuracy: 0.9714\n",
            "Epoch 92/100, Loss: 0.1180, Accuracy: 0.9714\n",
            "Epoch 93/100, Loss: 0.1168, Accuracy: 0.9714\n",
            "Epoch 94/100, Loss: 0.1156, Accuracy: 0.9736\n",
            "Epoch 95/100, Loss: 0.1145, Accuracy: 0.9780\n",
            "Epoch 96/100, Loss: 0.1134, Accuracy: 0.9780\n",
            "Epoch 97/100, Loss: 0.1123, Accuracy: 0.9780\n",
            "Epoch 98/100, Loss: 0.1113, Accuracy: 0.9780\n",
            "Epoch 99/100, Loss: 0.1102, Accuracy: 0.9780\n",
            "Epoch 100/100, Loss: 0.1092, Accuracy: 0.9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Neural Network**"
      ],
      "metadata": {
        "id": "mQbTWmMOOx4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  output = model(x_test_std)\n",
        "  predicted = output.round()\n",
        "  accuracy = (predicted == y_test_std.view(-1,1)).float().mean()\n",
        "  print(f'Test Accuracy: {accuracy.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP6OiavFO1mv",
        "outputId": "ef9e8bc0-4b6c-4369-b4f2-f5558a4fe93d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given new cases\n",
        "input_data = (20.57,17.77,132.9,1326,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667,0.5435,0.7339,3.398,74.08,0.005225,0.01308,0.0186,0.0134,0.01389,0.003532,24.99,23.41,158.8,1956,0.1238,0.1866,0.2416,0.186,0.275,0.08902)\n",
        "#input_data = (11.76,21.6,74.72,427.9,0.08637,0.04966,0.01657,0.01115,0.1495,0.05888,0.4062,1.21,2.635,28.47,0.005857,0.009758,0.01168,0.007445,0.02406,0.001769,12.98,25.72,82.98,516.5,0.1085,0.08615,0.05523,0.03715,0.2433,0.06563)\n",
        "\n",
        "import numpy as np\n",
        "input_np = np.asarray(input_data).reshape(1, -1) # change data to numPy and reshape it to (1,30)\n",
        "input_std = scaler.transform(input_np) # transorm() require two dimentional (num sample, num feature)\n",
        "\n",
        "input_tensor = torch.tensor(input_std, dtype=torch.float32).view(1, -1).to(device)\n",
        "with torch.no_grad():\n",
        "  output = model(input_tensor)\n",
        "  predicted = output.round()\n",
        "  if predicted.item() == 0:\n",
        "    print('The output is: Malignant')\n",
        "  else:\n",
        "    print('The output is: Benign')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqgTbJDDQNXV",
        "outputId": "c8fbd90a-b5cf-48a7-e939-4ce669a32ac8"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output is: Malignant\n"
          ]
        }
      ]
    }
  ]
}